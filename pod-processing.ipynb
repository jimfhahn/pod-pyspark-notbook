{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Step 1: Define Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing\") \\\n",
    "    .setMaster(\"local[4]\") \\\n",
    "    .set(\"spark.executor.memory\", \"300g\") \\\n",
    "    .set(\"spark.driver.memory\", \"300g\")\n",
    "\n",
    "# Step 2: Initialize SparkContext with the Configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Step 3: Initialize SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Now you can continue with your Spark operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install pymarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install marctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + '/home/jovyan/.local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pymarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import logging\n",
    "from pymarc import MARCReader, Record\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_files():\n",
    "    # Get a list of all MARC files\n",
    "    files = glob.glob('/home/jovyan/work/stanford-2024-08-28-full-marc21.mrc', recursive=True)\n",
    "    logger.info(f\"Found {len(files)} marc files\")\n",
    "    return files\n",
    "\n",
    "def process_file(file):\n",
    "    logger.info(f\"Processing file {file}\")\n",
    "\n",
    "    # Define the output directory for Parquet files\n",
    "    output_dir = '/home/jovyan/work/marc/parquet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a temporary file to store valid MARC records\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating temporary file for {file}: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Process the file in chunks\n",
    "    try:\n",
    "        with open(file, 'rb') as f_in, open(temp_file, 'wb') as temp_out:\n",
    "            if file.endswith('.xml'):\n",
    "                reader = XMLReader(f_in)\n",
    "            else:\n",
    "                reader = MARCReader(f_in)\n",
    "            for record in reader:\n",
    "                if not isinstance(record, Record):\n",
    "                    raise ValueError(\"Invalid MARC record\")\n",
    "                temp_out.write(record.as_marc())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file}: {e}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "\n",
    "    # Run the marctable command\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "    logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "    exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "    if exit_status != 0:\n",
    "        logger.error(f\"Error executing marctable command for file {file}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "    else:\n",
    "        logger.info(f\"Created Parquet file {output_file}\")\n",
    "\n",
    "    # Delete the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return True\n",
    "\n",
    "def marc2parquet():\n",
    "    files = get_files()\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        # Check if the corresponding Parquet file already exists\n",
    "        output_file = os.path.join('/home/jovyan/work/marc/parquet', os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "        if os.path.exists(output_file):\n",
    "            logger.info(f\"Skipping already processed file {file}\")\n",
    "            continue\n",
    "\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "\n",
    "    successful_files = sum(results)\n",
    "    logger.info(f\"Processed {len(results)} files, {successful_files} successful, {len(results) - successful_files} failed\")\n",
    "\n",
    "    # Return True if all files were processed successfully, otherwise False\n",
    "    return all(results)\n",
    "\n",
    "# Run the function and capture the result\n",
    "result = marc2parquet()\n",
    "logger.info(f\"marc2parquet result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_penn = spark.read.parquet('/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet')\n",
    "spark_df_brown = spark.read.parquet('/home/jovyan/work/marc/parquet/brown-2022-06-14-full-marc21.parquet')\n",
    "spark_df_chicago = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "spark_df_dartmouth = spark.read.parquet('/home/jovyan/work/marc/parquet/dartmouth-2022-08-19-full-marc21.parquet)\n",
    "spark_df_cornell = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "spark_df_jhu = spark.read.parquet('/home/jovyan/work/marc/parquet/jhu-2023-08-23-full-marc21.parquet')\n",
    "spark_df_mit = spark.read.parquet('/home/jovyan/work/marc/parquet/mit-marc21.parquet')\n",
    "spark_df_princeton = spark.read.parquet('/home/jovyan/work/marc/parquet/princeton-2022-06-17-full-marc21.parquet')\n",
    "spark_df_yale = spark.read.parquet('/home/jovyan/work/marc/parquet/yale-2022-06-17-full-marc21.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for unique records across libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spark sql, I need to query a field \n",
    "# and compare the two dataframes to find out how many records are common between \n",
    "# the two dataframes and how many are unique to each dataframe\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# Query the F035 field to get the OCLC number\n",
    "penn_oclc = spark.sql(\"SELECT F245 FROM penn\")\n",
    "brown_oclc = spark.sql(\"SELECT F245 FROM brown\")\n",
    "chicago_oclc = spark.sql(\"SELECT F245 FROM chicago\")\n",
    "cornell_oclc = spark.sql(\"SELECT F245 FROM cornell\")\n",
    "jhu_oclc = spark.sql(\"SELECT F245 FROM jhu\")\n",
    "mit_oclc = spark.sql(\"SELECT F245 FROM mit\")\n",
    "princeton_oclc = spark.sql(\"SELECT F245 FROM princeton\")\n",
    "yale_oclc = spark.sql(\"SELECT F245 FROM yale\")\n",
    "\n",
    "\n",
    "# Find the number of common records in each dataframe\n",
    "common_oclc = penn_oclc.intersect(brown_oclc)\n",
    "common_oclc = common_oclc.intersect(chicago_oclc)\n",
    "common_oclc = common_oclc.intersect(cornell_oclc)\n",
    "common_oclc = common_oclc.intersect(jhu_oclc)\n",
    "common_oclc = common_oclc.intersect(mit_oclc)\n",
    "common_oclc = common_oclc.intersect(princeton_oclc)\n",
    "common_oclc = common_oclc.intersect(yale_oclc)\n",
    "common_oclc.count()\n",
    "\n",
    "# Find the number of unique records in each dataframe\n",
    "unique_penn = penn_oclc.subtract(common_oclc)\n",
    "unique_brown = brown_oclc.subtract(common_oclc)\n",
    "unique_chicago = chicago_oclc.subtract(common_oclc)\n",
    "unique_cornell = cornell_oclc.subtract(common_oclc)\n",
    "unique_jhu = jhu_oclc.subtract(common_oclc)\n",
    "unique_mit = mit_oclc.subtract(common_oclc)\n",
    "unique_princeton = princeton_oclc.subtract(common_oclc)\n",
    "unique_yale = yale_oclc.subtract(common_oclc)\n",
    "\n",
    "unique_penn.count(), unique_brown.count(), unique_chicago.count(), unique_cornell.count(), unique_jhu.count(), unique_mit.count(), unique_princeton.count(), unique_yale.count()\n",
    "\n",
    "# print the results\n",
    "print(f\"Number of common records: {common_oclc.count()}\")\n",
    "print(f\"Number of unique records in Penn: {unique_penn.count()}\")\n",
    "print(f\"Number of unique records in Brown: {unique_brown.count()}\")\n",
    "print(f\"Number of unique records in Chicago: {unique_chicago.count()}\")\n",
    "print(f\"Number of unique records in Cornell: {unique_cornell.count()}\")\n",
    "print(f\"Number of unique records in JHU: {unique_jhu.count()}\")\n",
    "print(f\"Number of unique records in MIT: {unique_mit.count()}\")\n",
    "print(f\"Number of unique records in Princeton: {unique_princeton.count()}\")\n",
    "print(f\"Number of unique records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some trial SQL queries (older)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_df.createOrReplaceTempView(\"df\")\n",
    "df_001 = spark.sql(\"SELECT `001` FROM df\")\n",
    "# are there any null values?\n",
    "df_001.filter(df_001['001'].isNull()).count()\n",
    "#check if all records have 001 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#approximate number of items that are portuguese language\n",
    "por = spark.sql(\"SELECT * FROM df WHERE `008` LIKE '%por%'\")\n",
    "count_por = por.count()\n",
    "print(count_por)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
