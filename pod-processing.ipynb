{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Step 1: Define Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing\") \\\n",
    "    .setMaster(\"local[35]\") \\\n",
    "    .set(\"spark.executor.memory\", \"350g\") \\\n",
    "    .set(\"spark.driver.memory\", \"350g\")\n",
    "\n",
    "# Step 2: Initialize SparkContext with the Configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Step 3: Initialize SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install pymarc poetry marctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + '/home/jovyan/.local/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load only\n",
    "Converts MARC to Parquet format for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import logging\n",
    "from pymarc import Record\n",
    "from pymarc.marcxml import parse_xml_to_array\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_files():\n",
    "    # Use recursive search to pick up .xml files anywhere under the given directory.\n",
    "    # Filter out files that are not MARC XML (e.g. those containing 'opensearch' in their name)\n",
    "    all_files = glob.glob('/home/jovyan/work/*.xml', recursive=True)\n",
    "    files = [f for f in all_files if 'opensearch' not in os.path.basename(f).lower()]\n",
    "    logger.info(f\"Found {len(files)} marc files: {files}\")\n",
    "    return files\n",
    "\n",
    "def process_file(file):\n",
    "    logger.info(f\"Processing file {file}\")\n",
    "\n",
    "    # Define the output directory for Parquet files\n",
    "    output_dir = '/home/jovyan/work/marc/parquet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Determine the output file name: replace .xml with .parquet\n",
    "    base = os.path.basename(file)\n",
    "    output_file = os.path.join(output_dir, base.replace('.xml', '.parquet'))\n",
    "\n",
    "    if file.endswith('.xml'):\n",
    "        # Convert XML to MARC (MRC) format first using parse_xml_to_array.\n",
    "        try:\n",
    "            # Open XML file using utf-8 encoding instead of binary mode.\n",
    "            with open(file, 'r', encoding='utf-8') as f_in:\n",
    "                xml_string = f_in.read()\n",
    "            # Encode to bytes if the parser requires bytes input.\n",
    "            records = parse_xml_to_array(xml_string.encode('utf-8'))\n",
    "            logger.info(f\"Parsed {len(records)} MARC records from XML\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing XML file {file} with utf-8 encoding: {e}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "                temp_file = temp.name\n",
    "            with open(temp_file, 'wb') as f_temp:\n",
    "                for record in records:\n",
    "                    if not isinstance(record, Record):\n",
    "                        raise ValueError(\"Invalid MARC record\")\n",
    "                    f_temp.write(record.as_marc())\n",
    "            logger.info(f\"Wrote temporary MRC file {temp_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting XML to MRC for file {file}: {e}\")\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "            return False\n",
    "\n",
    "        # Run marctable on the temporary MRC file.\n",
    "        logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "        exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "        if exit_status != 0:\n",
    "            logger.error(f\"Error executing marctable command for file {file}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(f\"Created Parquet file {output_file}\")\n",
    "        os.remove(temp_file)\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        # Process non-XML files (for example, raw MRC files).\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "                temp_file = temp.name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating temporary file for {file}: {e}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(file, 'rb') as f_in, open(temp_file, 'wb') as temp_out:\n",
    "                from pymarc import MARCReader\n",
    "                reader = MARCReader(f_in, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "                for record in reader:\n",
    "                    if not isinstance(record, Record):\n",
    "                        raise ValueError(\"Invalid MARC record\")\n",
    "                    temp_out.write(record.as_marc())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {file}: {e}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "        exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "        if exit_status != 0:\n",
    "            logger.error(f\"Error executing marctable command for file {file}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(f\"Created Parquet file {output_file}\")\n",
    "\n",
    "        os.remove(temp_file)\n",
    "        return True\n",
    "\n",
    "def marc2parquet():\n",
    "    files = get_files()\n",
    "    results = []\n",
    "    for file in files:\n",
    "        # Determine output file name based on file extension.\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join('/home/jovyan/work/marc/parquet', base.replace('.xml', '.parquet'))\n",
    "        if os.path.exists(output_file):\n",
    "            logger.info(f\"Skipping already processed file {file}\")\n",
    "            continue\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "    successful_files = sum(results)\n",
    "    logger.info(f\"Processed {len(results)} files, {successful_files} successful, {len(results) - successful_files} failed\")\n",
    "    return all(results)\n",
    "\n",
    "# Run the function and capture the result\n",
    "result = marc2parquet()\n",
    "logger.info(f\"marc2parquet result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet\n",
    "The main benefits of Parquet (like predicate pushdown and column pruning) are realized during file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_penn = spark.read.parquet('/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet')\n",
    "spark_df_brown = spark.read.parquet('/home/jovyan/work/marc/parquet/brown-2022-06-14-full-marc21.parquet')\n",
    "spark_df_chicago = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "spark_df_columbia = spark.read.parquet('/home/jovyan/work/marc/parquet/columbia.parquet')\n",
    "spark_df_cornell = spark.read.parquet('/home/jovyan/work/marc/parquet/cornell-2023-04-28-full-marc21.parquet')\n",
    "spark_df_dartmouth = spark.read.parquet('/home/jovyan/work/marc/parquet/dartmouth-2022-08-19-full-marc21.parquet')\n",
    "spark_df_duke = spark.read.parquet('/home/jovyan/work/marc/parquet/duke-2025-01-29-full-marc21.parquet_clean_rev.parquet')\n",
    "spark_df_harvard = spark.read.parquet('/home/jovyan/work/marc/parquet/harvard.parquet')\n",
    "spark_df_jhu = spark.read.parquet('/home/jovyan/work/marc/parquet/jhu-2023-08-23-full-marc21.parquet')\n",
    "spark_df_mit = spark.read.parquet('/home/jovyan/work/marc/parquet/mit-marc21.parquet')\n",
    "spark_df_princeton = spark.read.parquet('/home/jovyan/work/marc/parquet/princeton-2022-06-17-full-marc21.parquet')\n",
    "spark_df_stanford = spark.read.parquet('/home/jovyan/work/marc/parquet/stanford-2024-08-28-full-marc21.parquet')\n",
    "spark_df_yale = spark.read.parquet('/home/jovyan/work/marc/parquet/yale-2022-06-17-full-marc21.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import matching libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find unique by Matchkey and lccn / isbn\n",
    "Matchkey: 1. normalized title 2. edition and 3. publication fields. Matchkey is used when 010 (lccn) or 020 (isbn) fields are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType, ArrayType\n",
    "from pyspark.sql.functions import udf, col, collect_set, explode\n",
    "import re\n",
    "import unicodedata\n",
    "from langdetect import detect\n",
    "\n",
    "# 1. Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "spark_df_columbia.createOrReplaceTempView(\"columbia\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_duke.createOrReplaceTempView(\"duke\")\n",
    "spark_df_harvard.createOrReplaceTempView(\"harvard\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# 2. Query to get fields from each view.\n",
    "# Include F010 (LCCN), F020 (ISBN), F245, F250 (edition), and F260 (publication/distribution).\n",
    "penn_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM penn\")\n",
    "brown_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM brown\")\n",
    "chicago_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM chicago\")\n",
    "columbia_245 = spark.sql(\"SELECT F001, F010, F020, F245, F250, F260 FROM columbia\")\n",
    "cornell_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM cornell\")\n",
    "dartmouth_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM dartmouth\")\n",
    "duke_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM duke\")\n",
    "harvard_245 = spark.sql(\"SELECT F001, F010, F020, F245, F250, F260 FROM harvard\")\n",
    "jhu_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM jhu\")\n",
    "mit_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM mit\")\n",
    "princeton_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM princeton\")\n",
    "stanford_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM stanford\")\n",
    "yale_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM yale\")\n",
    "\n",
    "# 3. Normalize titles (F245), edition (F250), and publication/distribution (F260)\n",
    "# Remove diacritics, punctuation, lowercase, and extra whitespace.\n",
    "def normalize(text):\n",
    "    if text:\n",
    "        # If text is a list, join its elements into a string.\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "        except Exception:\n",
    "            lang = \"unknown\"\n",
    "        if lang != \"en\":\n",
    "            # For non-English, keep diacritics; only lowercase and trim whitespace.\n",
    "            text = text.lower().strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "        else:\n",
    "            # English: remove diacritics and punctuation.\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "            text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "            text = text.lower().strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "    return None\n",
    "\n",
    "normalize_udf = udf(normalize, StringType())\n",
    "\n",
    "def normalized_df(df):\n",
    "    return df.withColumn(\"normalized_title\", normalize_udf(F.col(\"F245\"))) \\\n",
    "             .withColumn(\"normalized_edition\", normalize_udf(F.col(\"F250\"))) \\\n",
    "             .withColumn(\"normalized_pub\", normalize_udf(F.col(\"F260\")))\n",
    "\n",
    "# Apply normalization to each DataFrame\n",
    "penn_245 = normalized_df(penn_245)\n",
    "brown_245 = normalized_df(brown_245)\n",
    "chicago_245 = normalized_df(chicago_245)\n",
    "columbia_245 = normalized_df(columbia_245)\n",
    "cornell_245 = normalized_df(cornell_245)\n",
    "dartmouth_245 = normalized_df(dartmouth_245)\n",
    "duke_245 = normalized_df(duke_245)\n",
    "harvard_245 = normalized_df(harvard_245)\n",
    "jhu_245 = normalized_df(jhu_245)\n",
    "mit_245 = normalized_df(mit_245)\n",
    "princeton_245 = normalized_df(princeton_245)\n",
    "stanford_245 = normalized_df(stanford_245)\n",
    "yale_245 = normalized_df(yale_245)\n",
    "\n",
    "# 4. Create a match key from the normalized title, edition, and publication/distribution.\n",
    "def create_match_key(title, edition, pub):\n",
    "    if title:\n",
    "        key_parts = [title]\n",
    "        if edition:\n",
    "            key_parts.append(edition)\n",
    "        if pub:\n",
    "            key_parts.append(pub)\n",
    "        return \"_\".join(key_parts)\n",
    "    return None\n",
    "\n",
    "match_key_udf = udf(create_match_key, StringType())\n",
    "\n",
    "# 5. Add source column and a match key column\n",
    "penn_df = penn_245.withColumn(\"source\", F.lit(\"penn\")) \\\n",
    "                  .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "brown_df = brown_245.withColumn(\"source\", F.lit(\"brown\")) \\\n",
    "                    .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "chicago_df = chicago_245.withColumn(\"source\", F.lit(\"chicago\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "columbia_df = columbia_245.withColumn(\"source\", F.lit(\"columbia\")) \\\n",
    "                          .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "cornell_df = cornell_245.withColumn(\"source\", F.lit(\"cornell\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "dartmouth_df = dartmouth_245.withColumn(\"source\", F.lit(\"dartmouth\")) \\\n",
    "                            .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "duke_df = duke_245.withColumn(\"source\", F.lit(\"duke\")) \\\n",
    "                  .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "harvard_df = harvard_245.withColumn(\"source\", F.lit(\"harvard\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "jhu_df = jhu_245.withColumn(\"source\", F.lit(\"jhu\")) \\\n",
    "                .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "mit_df = mit_245.withColumn(\"source\", F.lit(\"mit\")) \\\n",
    "                .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "princeton_df = princeton_245.withColumn(\"source\", F.lit(\"princeton\")) \\\n",
    "                            .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "stanford_df = stanford_245.withColumn(\"source\", F.lit(\"stanford\")) \\\n",
    "                           .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "yale_df = yale_245.withColumn(\"source\", F.lit(\"yale\")) \\\n",
    "                   .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "\n",
    "# 6. Union all DataFrames\n",
    "all_df = (penn_df.union(brown_df).union(chicago_df).union(cornell_df)\n",
    "          .union(dartmouth_df).union(jhu_df).union(mit_df).union(princeton_df)\n",
    "          .union(stanford_df).union(yale_df).union(duke_df))\n",
    "\n",
    "# 7. Create a helper UDF to extract ISBN and LCCN values as a trimmed array.\n",
    "def get_ids(f010, f020):\n",
    "    ret = []\n",
    "    # Process F010\n",
    "    if f010:\n",
    "        if isinstance(f010, list):\n",
    "            for item in f010:\n",
    "                if item and isinstance(item, str):\n",
    "                    trimmed = item.strip()\n",
    "                    if trimmed:\n",
    "                        ret.append(trimmed)\n",
    "        elif isinstance(f010, str):\n",
    "            trimmed = f010.strip()\n",
    "            if trimmed:\n",
    "                ret.append(trimmed)\n",
    "    # Process F020\n",
    "    if f020:\n",
    "        if isinstance(f020, list):\n",
    "            for item in f020:\n",
    "                if item and isinstance(item, str):\n",
    "                    trimmed = item.strip()\n",
    "                    if trimmed:\n",
    "                        ret.append(trimmed)\n",
    "        elif isinstance(f020, str):\n",
    "            trimmed = f020.strip()\n",
    "            if trimmed:\n",
    "                ret.append(trimmed)\n",
    "    return ret\n",
    "\n",
    "get_ids_udf = udf(get_ids, ArrayType(StringType()))\n",
    "\n",
    "# 8. Add an id_list column (with ISBN and/or LCCN) to all records.\n",
    "all_df = all_df.withColumn(\"id_list\", get_ids_udf(F.col(\"F010\"), F.col(\"F020\")))\n",
    "\n",
    "# 9. Define the matching key array.\n",
    "# If any ISBN or LCCN identifiers exist, use them; otherwise, use the match key.\n",
    "all_df = all_df.withColumn(\"key_array\",\n",
    "    F.when(F.size(F.col(\"id_list\")) > 0, F.col(\"id_list\"))\n",
    "     .otherwise(F.array(F.col(\"match_key\")))\n",
    ")\n",
    "\n",
    "# 10. Explode the key_array so that each record has one key per identifier (or one key if using the match key).\n",
    "all_df_exploded = all_df.withColumn(\"key\", explode(\"key_array\"))\n",
    "\n",
    "# 11. Group by key and collect sources where that key appears.\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# 12. Define the complete set of sources.\n",
    "all_sources = {\"penn\", \"brown\", \"chicago\", \"cornell\", \"dartmouth\",\n",
    "               \"duke\", \"jhu\", \"mit\", \"princeton\", \"stanford\", \"yale\"}\n",
    "\n",
    "# 13. Identify common records: groups where the key appears in all sources.\n",
    "def has_all_sources(sources):\n",
    "    return all(src in sources for src in all_sources)\n",
    "\n",
    "has_all_sources_udf = udf(has_all_sources, BooleanType())\n",
    "common_fuzzy = grouped.filter(has_all_sources_udf(F.col(\"sources\")))\n",
    "\n",
    "# 14. Count common records.\n",
    "common_fuzzy_count = common_fuzzy.count()\n",
    "print(f\"Number of fuzzy common (all sources) records: {common_fuzzy_count}\")\n",
    "\n",
    "# 15. Derive unique records:\n",
    "# Perform an anti-join to remove records that are common across all sources,\n",
    "# then deduplicate based on the match_key so that only one copy of the same book remains.\n",
    "unique_exploded = all_df_exploded.join(common_fuzzy.select(\"key\"), on=\"key\", how=\"left_anti\")\n",
    "unique_df = unique_exploded.dropDuplicates([\"match_key\"])\n",
    "\n",
    "# 16. Count unique records per source.\n",
    "unique_penn = unique_df.filter(F.col(\"source\") == \"penn\")\n",
    "print(f\"Number of unique fuzzy records in Penn: {unique_penn.count()}\")\n",
    "unique_brown = unique_df.filter(F.col(\"source\") == \"brown\")\n",
    "print(f\"Number of unique fuzzy records in Brown: {unique_brown.count()}\")\n",
    "unique_chicago = unique_df.filter(F.col(\"source\") == \"chicago\")\n",
    "print(f\"Number of unique fuzzy records in Chicago: {unique_chicago.count()}\")\n",
    "unique_columbia = unique_df.filter(F.col(\"source\") == \"columbia\")\n",
    "print(f\"Number of unique fuzzy records in Columbia: {unique_columbia.count()}\")\n",
    "unique_cornell = unique_df.filter(F.col(\"source\") == \"cornell\")\n",
    "print(f\"Number of unique fuzzy records in Cornell: {unique_cornell.count()}\")\n",
    "unique_dartmouth = unique_df.filter(F.col(\"source\") == \"dartmouth\")\n",
    "print(f\"Number of unique fuzzy records in Dartmouth: {unique_dartmouth.count()}\")\n",
    "unique_duke = unique_df.filter(F.col(\"source\") == \"duke\")\n",
    "print(f\"Number of unique fuzzy records in Duke: {unique_duke.count()}\")\n",
    "unique_harvard = unique_df.filter(F.col(\"source\") == \"harvard\")\n",
    "print(f\"Number of unique fuzzy records in Harvard: {unique_harvard.count()}\")\n",
    "unique_jhu = unique_df.filter(F.col(\"source\") == \"jhu\")\n",
    "print(f\"Number of unique fuzzy records in JHU: {unique_jhu.count()}\")\n",
    "unique_mit = unique_df.filter(F.col(\"source\") == \"mit\")\n",
    "print(f\"Number of unique fuzzy records in MIT: {unique_mit.count()}\")\n",
    "unique_princeton = unique_df.filter(F.col(\"source\") == \"princeton\")\n",
    "print(f\"Number of unique fuzzy records in Princeton: {unique_princeton.count()}\")\n",
    "unique_stanford = unique_df.filter(F.col(\"source\") == \"stanford\")\n",
    "print(f\"Number of unique fuzzy records in Stanford: {unique_stanford.count()}\")\n",
    "unique_yale = unique_df.filter(F.col(\"source\") == \"yale\")\n",
    "print(f\"Number of unique fuzzy records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "# 17. Save results\n",
    "unique_df.select(\"F001\", \"F245\", \"source\").write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/home/jovyan/work/marc/unique_fuzzy_records.parquet\")\n",
    "common_fuzzy.select(\"key\", \"sources\", \"record_count\").write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/home/jovyan/work/marc/common_fuzzy_records.parquet\")\n",
    "unique_penn.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")\n",
    "unique_brown.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_brown.parquet\")\n",
    "unique_chicago.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_chicago.parquet\")\n",
    "unique_columbia.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_columbia.parquet\")\n",
    "unique_cornell.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_cornell.parquet\")\n",
    "unique_dartmouth.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_dartmouth.parquet\")\n",
    "unique_duke.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_duke.parquet\")\n",
    "unique_harvard.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_harvard.parquet\")\n",
    "unique_jhu.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_jhu.parquet\")\n",
    "unique_mit.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_mit.parquet\")\n",
    "unique_princeton.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_princeton.parquet\")\n",
    "unique_stanford.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_stanford.parquet\")\n",
    "unique_yale.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_yale.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the unique records from penn\n",
    "unique_penn = spark.read.parquet('/home/jovyan/work/marc/unique_penn.parquet')\n",
    "\n",
    "# Create temp view\n",
    "unique_penn.createOrReplaceTempView(\"unique_penn\")\n",
    "\n",
    "# we are going to add a column to support sorting by 007 format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique records are there in the Penn rarest titles?\n",
    "print(f\"Number of unique records in Penn rarest titles: {rarest_titles.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude electronic resources\n",
    "unique_print_penn = unique_penn.filter(F.substring(F.col(\"F007\").getItem(0), 1, 1) != \"c\")\n",
    "\n",
    "# Further include only records where F007/00 equals \"t\" (identifies the item physically as a text)\n",
    "unique_text_print_penn = unique_print_penn.filter(F.substring(F.col(\"F007\").getItem(0), 1, 1) == \"t\")\n",
    "\n",
    "# Now select the rarest titles (ordering by F245 here is only illustrative; adjust as needed)\n",
    "rarest_titles = unique_text_print_penn.select(\"F001\", \"F245\").orderBy(F.col(\"F245\").desc())\n",
    "rarest_titles.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude electronic resources\n",
    "unique_print_penn = unique_penn.filter(F.substring(F.col(\"F007\").getItem(0), 1, 1) != \"c\")\n",
    "\n",
    "# Further include only records where F007/00 equals \"t\" (identifies the item physically as a text)\n",
    "unique_text_print_penn = unique_print_penn.filter(F.substring(F.col(\"F007\").getItem(0), 1, 1) == \"t\")\n",
    "\n",
    "# Now select the rarest titles (ordering by F245 here is only illustrative; adjust as needed)\n",
    "rarest_titles = unique_text_print_penn.select(\"F001\", \"F245\").orderBy(F.col(\"F245\").desc())\n",
    "rarest_titles.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique records in the Penn rarest text titles\n",
    "print(f\"Number of unique records in Penn rarest titles: {rarest_titles.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export excel file \n",
    "rarest_titles.toPandas().to_excel(\"/home/jovyan/work/marc/penn_rarest_titles_text.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
