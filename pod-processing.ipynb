{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Step 1: Define Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing\") \\\n",
    "    .setMaster(\"local[4]\") \\\n",
    "    .set(\"spark.executor.memory\", \"350g\") \\\n",
    "    .set(\"spark.driver.memory\", \"350g\")\n",
    "\n",
    "# Step 2: Initialize SparkContext with the Configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Step 3: Initialize SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install pymarc poetry marctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + '/home/jovyan/.local/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import logging\n",
    "from pymarc import MARCReader, Record, XMLReader\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_files():\n",
    "    # Get a list of all MARC files\n",
    "    files = glob.glob('/home/jovyan/work/newmarc/*.mrc_clean_rev.mrc', recursive=True)\n",
    "    logger.info(f\"Found {len(files)} marc files\")\n",
    "    return files\n",
    "\n",
    "def process_file(file):\n",
    "    logger.info(f\"Processing file {file}\")\n",
    "\n",
    "    # Define the output directory for Parquet files\n",
    "    output_dir = '/home/jovyan/work/marc/parquet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a temporary file to store valid MARC records\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating temporary file for {file}: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Process the file in chunks\n",
    "    try:\n",
    "        with open(file, 'rb') as f_in, open(temp_file, 'wb') as temp_out:\n",
    "            if file.endswith('.xml'):\n",
    "                reader = XMLReader(f_in)\n",
    "            else:\n",
    "                reader = MARCReader(f_in, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            for record in reader:\n",
    "                if not isinstance(record, Record):\n",
    "                    raise ValueError(\"Invalid MARC record\")\n",
    "                temp_out.write(record.as_marc())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file}: {e}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "\n",
    "    # Run the marctable command\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "    logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "    exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "    if exit_status != 0:\n",
    "        logger.error(f\"Error executing marctable command for file {file}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "    else:\n",
    "        logger.info(f\"Created Parquet file {output_file}\")\n",
    "\n",
    "    # Delete the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return True\n",
    "\n",
    "def marc2parquet():\n",
    "    files = get_files()\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        # Check if the corresponding Parquet file already exists\n",
    "        output_file = os.path.join('/home/jovyan/work/marc/parquet', os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "        if os.path.exists(output_file):\n",
    "            logger.info(f\"Skipping already processed file {file}\")\n",
    "            continue\n",
    "\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "\n",
    "    successful_files = sum(results)\n",
    "    logger.info(f\"Processed {len(results)} files, {successful_files} successful, {len(results) - successful_files} failed\")\n",
    "\n",
    "    # Return True if all files were processed successfully, otherwise False\n",
    "    return all(results)\n",
    "\n",
    "# Run the function and capture the result\n",
    "result = marc2parquet()\n",
    "logger.info(f\"marc2parquet result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet\n",
    "The main benefits of Parquet (like predicate pushdown and column pruning) are realized during file I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_penn = spark.read.parquet('/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet')\n",
    "spark_df_brown = spark.read.parquet('/home/jovyan/work/marc/parquet/brown-2022-06-14-full-marc21.parquet')\n",
    "spark_df_chicago = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "# df_columbia = spark.read.parquet('/home/jovyan/work/marc/parquet/columbia-2022-06-17-full-marc21.parquet')\n",
    "spark_df_cornell = spark.read.parquet('/home/jovyan/work/marc/parquet/cornell-2023-04-28-full-marc21.parquet')\n",
    "spark_df_dartmouth = spark.read.parquet('/home/jovyan/work/marc/parquet/dartmouth-2022-08-19-full-marc21.parquet')\n",
    "spark_df_duke = spark.read.parquet('/home/jovyan/work/marc/parquet/duke-2025-01-29-full-marc21.parquet_clean_rev.parquet')\n",
    "# spark_df_harvard = spark.read.parquet('/home/jovyan/work/marc/parquet/harvard-2022-06-17-full-marc21.parquet')\n",
    "spark_df_jhu = spark.read.parquet('/home/jovyan/work/marc/parquet/jhu-2023-08-23-full-marc21.parquet')\n",
    "spark_df_mit = spark.read.parquet('/home/jovyan/work/marc/parquet/mit-marc21.parquet')\n",
    "spark_df_princeton = spark.read.parquet('/home/jovyan/work/marc/parquet/princeton-2022-06-17-full-marc21.parquet')\n",
    "spark_df_stanford = spark.read.parquet('/home/jovyan/work/marc/parquet/stanford-2024-08-28-full-marc21.parquet')\n",
    "spark_df_yale = spark.read.parquet('/home/jovyan/work/marc/parquet/yale-2022-06-17-full-marc21.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for unique records across libraries\n",
    "Querying marc record fields like columns in a sql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# Query to get F245 field\n",
    "penn_oclc = spark.sql(\"SELECT F245 FROM penn\")\n",
    "brown_oclc = spark.sql(\"SELECT F245 FROM brown\")\n",
    "chicago_oclc = spark.sql(\"SELECT F245 FROM chicago\")\n",
    "cornell_oclc = spark.sql(\"SELECT F245 FROM cornell\")\n",
    "dartmouth_oclc = spark.sql(\"SELECT F245 FROM dartmouth\")\n",
    "jhu_oclc = spark.sql(\"SELECT F245 FROM jhu\")\n",
    "mit_oclc = spark.sql(\"SELECT F245 FROM mit\")\n",
    "princeton_oclc = spark.sql(\"SELECT F245 FROM princeton\")\n",
    "stanford_oclc = spark.sql(\"SELECT F245 FROM stanford\")\n",
    "yale_oclc = spark.sql(\"SELECT F245 FROM yale\")\n",
    "\n",
    "# UDF for fuzzy similarity\n",
    "fuzzy_match_udf = udf(lambda x, y: fuzz.token_set_ratio(x, y) if x and y else 0, IntegerType())\n",
    "\n",
    "# Normalize titles\n",
    "def normalize_df(df):\n",
    "    return df.withColumn(\"normalized\", F.lower(F.regexp_replace(F.col(\"F245\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "penn_oclc = normalize_df(penn_oclc)\n",
    "brown_oclc = normalize_df(brown_oclc)\n",
    "chicago_oclc = normalize_df(chicago_oclc)\n",
    "cornell_oclc = normalize_df(cornell_oclc)\n",
    "dartmouth_oclc = normalize_df(dartmouth_oclc)\n",
    "jhu_oclc = normalize_df(jhu_oclc)\n",
    "mit_oclc = normalize_df(mit_oclc)\n",
    "princeton_oclc = normalize_df(princeton_oclc)\n",
    "stanford_oclc = normalize_df(stanford_oclc)\n",
    "yale_oclc = normalize_df(yale_oclc)\n",
    "\n",
    "# Exact common intersection\n",
    "common_oclc = (\n",
    "    penn_oclc.select(\"F245\")\n",
    "    .intersect(brown_oclc.select(\"F245\"))\n",
    "    .intersect(chicago_oclc.select(\"F245\"))\n",
    "    .intersect(cornell_oclc.select(\"F245\"))\n",
    "    .intersect(dartmouth_oclc.select(\"F245\"))\n",
    "    .intersect(jhu_oclc.select(\"F245\"))\n",
    "    .intersect(mit_oclc.select(\"F245\"))\n",
    "    .intersect(princeton_oclc.select(\"F245\"))\n",
    "    .intersect(stanford_oclc.select(\"F245\"))\n",
    "    .intersect(yale_oclc.select(\"F245\"))\n",
    ")\n",
    "\n",
    "unique_penn = penn_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_brown = brown_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_chicago = chicago_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_cornell = cornell_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_dartmouth = dartmouth_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_jhu = jhu_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_mit = mit_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_princeton = princeton_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_stanford = stanford_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_yale = yale_oclc.select(\"F245\").subtract(common_oclc)\n",
    "\n",
    "print(f\"Number of common records (exact): {common_oclc.count()}\")\n",
    "print(f\"Number of unique records in Penn: {unique_penn.count()}\")\n",
    "print(f\"Number of unique records in Brown: {unique_brown.count()}\")\n",
    "print(f\"Number of unique records in Chicago: {unique_chicago.count()}\")\n",
    "print(f\"Number of unique records in Cornell: {unique_cornell.count()}\")\n",
    "print(f\"Number of unique records in Dartmouth: {unique_dartmouth.count()}\")\n",
    "print(f\"Number of unique records in JHU: {unique_jhu.count()}\")\n",
    "print(f\"Number of unique records in MIT: {unique_mit.count()}\")\n",
    "print(f\"Number of unique records in Princeton: {unique_princeton.count()}\")\n",
    "print(f\"Number of unique records in Stanford: {unique_stanford.count()}\")\n",
    "print(f\"Number of unique records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "# Example fuzzy matching for Penn vs Brown (repeat for others as necessary):\n",
    "threshold = 85\n",
    "common_fuzzy_penn_brown = (\n",
    "    penn_oclc.alias(\"p\")\n",
    "    .crossJoin(brown_oclc.alias(\"b\"))\n",
    "    .withColumn(\"similarity\", fuzzy_match_udf(F.col(\"p.normalized\"), F.col(\"b.normalized\")))\n",
    "    .filter(F.col(\"similarity\") >= threshold)\n",
    ")\n",
    "print(f\"Fuzzy common count Penn-Brown: {common_fuzzy_penn_brown.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy match on title\n",
    "Query marc records for similar titles using fuzzy matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fuzzy common (all sources) records: 61819\n",
      "Number of unique fuzzy records in Penn: 6340243\n",
      "Number of unique fuzzy records in Brown: 2389662\n",
      "Number of unique fuzzy records in Chicago: 7286430\n",
      "Number of unique fuzzy records in Cornell: 5862754\n",
      "Number of unique fuzzy records in Dartmouth: 1581860\n",
      "Number of unique fuzzy records in JHU: 1631356\n",
      "Number of unique fuzzy records in MIT: 1335773\n",
      "Number of unique fuzzy records in Princeton: 5870534\n",
      "Number of unique fuzzy records in Stanford: 8068232\n",
      "Number of unique fuzzy records in Yale: 7432757\n",
      "Number of unique fuzzy records in Duke: 7360689\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.sql.functions import udf, col, collect_set\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# 1. Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "#spark_df_columbia.createOrReplaceTempView(\"columbia\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_duke.createOrReplaceTempView(\"duke\")\n",
    "#spark_df_harvard.createOrReplaceTempView(\"harvard\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# 2. Query to get 245 field from each table/view and drop duplicates\n",
    "penn_245 = spark.sql(\"SELECT F001, F245, F007 FROM penn\").dropDuplicates([\"F245\"])\n",
    "brown_245 = spark.sql(\"SELECT F001, F245, F007 FROM brown\").dropDuplicates([\"F245\"])\n",
    "chicago_245 = spark.sql(\"SELECT F001, F245, F007 FROM chicago\").dropDuplicates([\"F245\"])\n",
    "# columbia_245 = spark.sql(\"SELECT F001, F245 FROM columbia\").dropDuplicates([\"F245\"])\n",
    "cornell_245 = spark.sql(\"SELECT F001, F245, F007 FROM cornell\").dropDuplicates([\"F245\"])\n",
    "dartmouth_245 = spark.sql(\"SELECT F001, F245, F007 FROM dartmouth\").dropDuplicates([\"F245\"])\n",
    "duke_245 = spark.sql(\"SELECT F001, F245, F007 FROM duke\").dropDuplicates([\"F245\"])\n",
    "# harvard_245 = spark.sql(\"SELECT F001, F245 FROM harvard\").dropDuplicates([\"F245\"])\n",
    "jhu_245 = spark.sql(\"SELECT F001, F245, F007 FROM jhu\").dropDuplicates([\"F245\"])\n",
    "mit_245 = spark.sql(\"SELECT F001, F245, F007 FROM mit\").dropDuplicates([\"F245\"])\n",
    "princeton_245 = spark.sql(\"SELECT F001, F245, F007 FROM princeton\").dropDuplicates([\"F245\"])\n",
    "stanford_245 = spark.sql(\"SELECT F001, F245, F007 FROM stanford\").dropDuplicates([\"F245\"])\n",
    "yale_245 = spark.sql(\"SELECT F001, F245, F007 FROM yale\").dropDuplicates([\"F245\"])\n",
    "\n",
    "# 3. Normalize titles: remove diacritics, punctuation, lowercase, etc.\n",
    "def normalize(text):\n",
    "    if text:\n",
    "        # Normalize Unicode and remove diacritics\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "        # Remove punctuation (non-alphanumeric except whitespace)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Lowercase and strip extra whitespace\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "normalize_udf = udf(normalize, StringType())\n",
    "\n",
    "def normalized_df(df):\n",
    "    return df.withColumn(\"normalized\", normalize_udf(F.col(\"F245\")))\n",
    "\n",
    "penn_245 = normalized_df(penn_245)\n",
    "brown_245 = normalized_df(brown_245)\n",
    "chicago_245 = normalized_df(chicago_245)\n",
    "# columbia_245 = normalized_df(columbia_245)\n",
    "cornell_245 = normalized_df(cornell_245)\n",
    "dartmouth_245 = normalized_df(dartmouth_245)\n",
    "duke_245 = normalized_df(duke_245)\n",
    "# harvard_245 = normalized_df(harvard_245)\n",
    "jhu_245 = normalized_df(jhu_245)\n",
    "mit_245 = normalized_df(mit_245)\n",
    "princeton_245 = normalized_df(princeton_245)\n",
    "stanford_245 = normalized_df(stanford_245)\n",
    "yale_245 = normalized_df(yale_245)\n",
    "\n",
    "# 4. Create a simple fingerprint by sorting tokens\n",
    "def fingerprint(text):\n",
    "    if text:\n",
    "        tokens = text.split()\n",
    "        tokens.sort()\n",
    "        return \" \".join(tokens)\n",
    "    return None\n",
    "\n",
    "fingerprint_udf = udf(fingerprint, StringType())\n",
    "\n",
    "# 5. Add source columns and fingerprint column\n",
    "penn_df = penn_245.withColumn(\"source\", F.lit(\"penn\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "brown_df = brown_245.withColumn(\"source\", F.lit(\"brown\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "chicago_df = chicago_245.withColumn(\"source\", F.lit(\"chicago\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "# columbia_df = columbia_245.withColumn(\"source\", F.lit(\"columbia\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "cornell_df = cornell_245.withColumn(\"source\", F.lit(\"cornell\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "dartmouth_df = dartmouth_245.withColumn(\"source\", F.lit(\"dartmouth\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "duke_df = duke_245.withColumn(\"source\", F.lit(\"duke\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "# harvard_df = harvard_245.withColumn(\"source\", F.lit(\"harvard\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "jhu_df = jhu_245.withColumn(\"source\", F.lit(\"jhu\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "mit_df = mit_245.withColumn(\"source\", F.lit(\"mit\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "princeton_df = princeton_245.withColumn(\"source\", F.lit(\"princeton\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "stanford_df = stanford_245.withColumn(\"source\", F.lit(\"stanford\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "yale_df = yale_245.withColumn(\"source\", F.lit(\"yale\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "\n",
    "# 6. Union all DataFrames\n",
    "all_df = (penn_df.union(brown_df).union(chicago_df).union(cornell_df)\n",
    "          .union(dartmouth_df).union(jhu_df).union(mit_df).union(princeton_df)\n",
    "          .union(stanford_df).union(yale_df).union(duke_df))\n",
    "\n",
    "# 7. Group by fingerprint and collect the sources where that fingerprint appears\n",
    "grouped = all_df.groupBy(\"fp\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# 8. Define the list of all sources\n",
    "all_sources = {\"penn\", \"brown\", \"chicago\", \"cornell\", \"dartmouth\",\n",
    "               \"duke\", \"jhu\", \"mit\", \"princeton\", \"stanford\", \"yale\"}\n",
    "\n",
    "# 9. Identify fuzzy common records\n",
    "def has_all_sources(sources):\n",
    "    return set(sources) == all_sources\n",
    "\n",
    "has_all_sources_udf = udf(has_all_sources, BooleanType())\n",
    "common_fuzzy = grouped.filter(has_all_sources_udf(F.col(\"sources\")))\n",
    "\n",
    "# 10. Count fuzzy common records\n",
    "common_fuzzy_count = common_fuzzy.count()\n",
    "print(f\"Number of fuzzy common (all sources) records: {common_fuzzy_count}\")\n",
    "\n",
    "# 11. Derive unique records (records not in common fuzzy groups)\n",
    "unique_df = all_df.join(common_fuzzy.select(\"fp\"), on=\"fp\", how=\"left_anti\")\n",
    "\n",
    "# 12. Count unique records per source\n",
    "unique_penn = unique_df.filter(F.col(\"source\") == \"penn\")\n",
    "print(f\"Number of unique fuzzy records in Penn: {unique_penn.count()}\")\n",
    "\n",
    "unique_brown = unique_df.filter(F.col(\"source\") == \"brown\")\n",
    "print(f\"Number of unique fuzzy records in Brown: {unique_brown.count()}\")\n",
    "\n",
    "unique_chicago = unique_df.filter(F.col(\"source\") == \"chicago\")\n",
    "print(f\"Number of unique fuzzy records in Chicago: {unique_chicago.count()}\")\n",
    "\n",
    "unique_cornell = unique_df.filter(F.col(\"source\") == \"cornell\")\n",
    "print(f\"Number of unique fuzzy records in Cornell: {unique_cornell.count()}\")\n",
    "\n",
    "unique_dartmouth = unique_df.filter(F.col(\"source\") == \"dartmouth\")\n",
    "print(f\"Number of unique fuzzy records in Dartmouth: {unique_dartmouth.count()}\")\n",
    "\n",
    "unique_jhu = unique_df.filter(F.col(\"source\") == \"jhu\")\n",
    "print(f\"Number of unique fuzzy records in JHU: {unique_jhu.count()}\")\n",
    "\n",
    "unique_mit = unique_df.filter(F.col(\"source\") == \"mit\")\n",
    "print(f\"Number of unique fuzzy records in MIT: {unique_mit.count()}\")\n",
    "\n",
    "unique_princeton = unique_df.filter(F.col(\"source\") == \"princeton\")\n",
    "print(f\"Number of unique fuzzy records in Princeton: {unique_princeton.count()}\")\n",
    "\n",
    "unique_stanford = unique_df.filter(F.col(\"source\") == \"stanford\")\n",
    "print(f\"Number of unique fuzzy records in Stanford: {unique_stanford.count()}\")\n",
    "\n",
    "unique_yale = unique_df.filter(F.col(\"source\") == \"yale\")\n",
    "print(f\"Number of unique fuzzy records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "#unique_columbia = unique_df.filter(F.col(\"source\") == \"columbia\")\n",
    "#print(f\"Number of unique fuzzy records in Columbia: {unique_columbia.count()}\")\n",
    "\n",
    "unique_duke = unique_df.filter(F.col(\"source\") == \"duke\")\n",
    "print(f\"Number of unique fuzzy records in Duke: {unique_duke.count()}\")\n",
    "\n",
    "#unique_harvard = unique_df.filter(F.col(\"source\") == \"harvard\")\n",
    "#print(f\"Number of unique fuzzy records in Harvard: {unique_harvard.count()}\")\n",
    "\n",
    "# 13. Save the unique records to a file\n",
    "unique_df.select(\"F001\", \"F245\", \"source\").write.mode(\"overwrite\") \\\n",
    "  .parquet(\"/home/jovyan/work/marc/unique_fuzzy_records.parquet\")\n",
    "\n",
    "# 14. Save the common fuzzy records to a file\n",
    "common_fuzzy.select(\"fp\", \"sources\", \"record_count\").write.mode(\"overwrite\") \\\n",
    "  .parquet(\"/home/jovyan/work/marc/common_fuzzy_records.parquet\")\n",
    "\n",
    "# 15. Save the unique records per source to a file\n",
    "unique_penn.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")\n",
    "unique_brown.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_brown.parquet\")\n",
    "unique_chicago.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_chicago.parquet\")\n",
    "unique_cornell.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_cornell.parquet\")\n",
    "unique_dartmouth.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_dartmouth.parquet\")\n",
    "unique_jhu.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_jhu.parquet\")\n",
    "unique_mit.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_mit.parquet\")\n",
    "unique_princeton.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_princeton.parquet\")\n",
    "unique_stanford.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_stanford.parquet\")\n",
    "unique_yale.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_yale.parquet\")\n",
    "# unique_columbia.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_columbia.parquet\")\n",
    "unique_duke.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_duke.parquet\")\n",
    "# unique_harvard.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_harvard.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            F001|                F245|\n",
      "+----------------+--------------------+\n",
      "|9977609996603681|  к.ш.з. 1932, 1932.|\n",
      "|9923858713503681|ʾIgeret R. Yehosh...|\n",
      "|9939185833503681|ʾIgeret HaMaskil ...|\n",
      "|9941037913503681|ʻUmar qatalatū a...|\n",
      "|9915824013503681|ʻUbaidullāh Balo...|\n",
      "|9954002973503681|ʻOng bāk = Ong-B...|\n",
      "|9943206953503681|ʻImārat Yaʻqūbi...|\n",
      "|9941722713503681|ʻEts ha-domim taf...|\n",
      "|9930899413503681|ʻEts ha-domim taf...|\n",
      "|9954169053503681|ʻEli Mohar = Ali ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analyze the unique records from penn\n",
    "unique_penn = spark.read.parquet('/home/jovyan/work/marc/unique_penn.parquet')\n",
    "\n",
    "# Create temp view (if needed)\n",
    "unique_penn.createOrReplaceTempView(\"unique_penn\")\n",
    "\n",
    "# Exclude records where the first character of the first element in F007 is \"c\" (electronic resources)\n",
    "unique_print_penn = unique_penn.filter(F.substring(F.col(\"F007\").getItem(0), 1, 1) != \"c\")\n",
    "\n",
    "# Now select the rarest titles (ordering by F245 here is only illustrative; adjust as needed)\n",
    "rarest_titles = unique_print_penn.select(\"F001\", \"F245\").orderBy(F.col(\"F245\").desc())\n",
    "rarest_titles.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique records in Penn rarest titles: 350417\n"
     ]
    }
   ],
   "source": [
    "# how many unique records are there in the Penn rarest titles?\n",
    "print(f\"Number of unique records in Penn rarest titles: {rarest_titles.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put these rarest titles and F001 into an Excel file\n",
    "rarest_titles.toPandas().to_excel(\"/home/jovyan/work/marc/penn_rarest_titles.xlsx\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
