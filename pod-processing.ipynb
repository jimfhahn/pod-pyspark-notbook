{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Step 1: Define Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing\") \\\n",
    "    .setMaster(\"local[4]\") \\\n",
    "    .set(\"spark.executor.memory\", \"350g\") \\\n",
    "    .set(\"spark.driver.memory\", \"350g\")\n",
    "\n",
    "# Step 2: Initialize SparkContext with the Configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Step 3: Initialize SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install pymarc poetry marctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + '/home/jovyan/.local/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import logging\n",
    "from pymarc import MARCReader, Record, XMLReader\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_files():\n",
    "    # Get a list of all MARC files\n",
    "    files = glob.glob('/home/jovyan/work/newmarc/*.mrc_clean_rev.mrc', recursive=True)\n",
    "    logger.info(f\"Found {len(files)} marc files\")\n",
    "    return files\n",
    "\n",
    "def process_file(file):\n",
    "    logger.info(f\"Processing file {file}\")\n",
    "\n",
    "    # Define the output directory for Parquet files\n",
    "    output_dir = '/home/jovyan/work/marc/parquet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a temporary file to store valid MARC records\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating temporary file for {file}: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Process the file in chunks\n",
    "    try:\n",
    "        with open(file, 'rb') as f_in, open(temp_file, 'wb') as temp_out:\n",
    "            if file.endswith('.xml'):\n",
    "                reader = XMLReader(f_in)\n",
    "            else:\n",
    "                reader = MARCReader(f_in, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            for record in reader:\n",
    "                if not isinstance(record, Record):\n",
    "                    raise ValueError(\"Invalid MARC record\")\n",
    "                temp_out.write(record.as_marc())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file}: {e}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "\n",
    "    # Run the marctable command\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "    logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "    exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "    if exit_status != 0:\n",
    "        logger.error(f\"Error executing marctable command for file {file}\")\n",
    "        os.remove(temp_file)\n",
    "        return False\n",
    "    else:\n",
    "        logger.info(f\"Created Parquet file {output_file}\")\n",
    "\n",
    "    # Delete the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return True\n",
    "\n",
    "def marc2parquet():\n",
    "    files = get_files()\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        # Check if the corresponding Parquet file already exists\n",
    "        output_file = os.path.join('/home/jovyan/work/marc/parquet', os.path.basename(file).replace('.mrc', '.parquet'))\n",
    "        if os.path.exists(output_file):\n",
    "            logger.info(f\"Skipping already processed file {file}\")\n",
    "            continue\n",
    "\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "\n",
    "    successful_files = sum(results)\n",
    "    logger.info(f\"Processed {len(results)} files, {successful_files} successful, {len(results) - successful_files} failed\")\n",
    "\n",
    "    # Return True if all files were processed successfully, otherwise False\n",
    "    return all(results)\n",
    "\n",
    "# Run the function and capture the result\n",
    "result = marc2parquet()\n",
    "logger.info(f\"marc2parquet result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet\n",
    "The main benefits of Parquet (like predicate pushdown and column pruning) are realized during file I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_penn = spark.read.parquet('/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet')\n",
    "spark_df_brown = spark.read.parquet('/home/jovyan/work/marc/parquet/brown-2022-06-14-full-marc21.parquet')\n",
    "spark_df_chicago = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "# df_columbia = spark.read.parquet('/home/jovyan/work/marc/parquet/columbia-2022-06-17-full-marc21.parquet')\n",
    "spark_df_cornell = spark.read.parquet('/home/jovyan/work/marc/parquet/cornell-2023-04-28-full-marc21.parquet')\n",
    "spark_df_dartmouth = spark.read.parquet('/home/jovyan/work/marc/parquet/dartmouth-2022-08-19-full-marc21.parquet')\n",
    "spark_df_duke = spark.read.parquet('/home/jovyan/work/marc/parquet/duke-2025-01-29-full-marc21.parquet_clean_rev.parquet')\n",
    "# spark_df_harvard = spark.read.parquet('/home/jovyan/work/marc/parquet/harvard-2022-06-17-full-marc21.parquet')\n",
    "spark_df_jhu = spark.read.parquet('/home/jovyan/work/marc/parquet/jhu-2023-08-23-full-marc21.parquet')\n",
    "spark_df_mit = spark.read.parquet('/home/jovyan/work/marc/parquet/mit-marc21.parquet')\n",
    "spark_df_princeton = spark.read.parquet('/home/jovyan/work/marc/parquet/princeton-2022-06-17-full-marc21.parquet')\n",
    "spark_df_stanford = spark.read.parquet('/home/jovyan/work/marc/parquet/stanford-2024-08-28-full-marc21.parquet')\n",
    "spark_df_yale = spark.read.parquet('/home/jovyan/work/marc/parquet/yale-2022-06-17-full-marc21.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for unique records across libraries\n",
    "Querying marc record fields like columns in a sql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# Query to get F245 field\n",
    "penn_oclc = spark.sql(\"SELECT F245 FROM penn\")\n",
    "brown_oclc = spark.sql(\"SELECT F245 FROM brown\")\n",
    "chicago_oclc = spark.sql(\"SELECT F245 FROM chicago\")\n",
    "cornell_oclc = spark.sql(\"SELECT F245 FROM cornell\")\n",
    "dartmouth_oclc = spark.sql(\"SELECT F245 FROM dartmouth\")\n",
    "jhu_oclc = spark.sql(\"SELECT F245 FROM jhu\")\n",
    "mit_oclc = spark.sql(\"SELECT F245 FROM mit\")\n",
    "princeton_oclc = spark.sql(\"SELECT F245 FROM princeton\")\n",
    "stanford_oclc = spark.sql(\"SELECT F245 FROM stanford\")\n",
    "yale_oclc = spark.sql(\"SELECT F245 FROM yale\")\n",
    "\n",
    "# UDF for fuzzy similarity\n",
    "fuzzy_match_udf = udf(lambda x, y: fuzz.token_set_ratio(x, y) if x and y else 0, IntegerType())\n",
    "\n",
    "# Normalize titles\n",
    "def normalize_df(df):\n",
    "    return df.withColumn(\"normalized\", F.lower(F.regexp_replace(F.col(\"F245\"), r\"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "penn_oclc = normalize_df(penn_oclc)\n",
    "brown_oclc = normalize_df(brown_oclc)\n",
    "chicago_oclc = normalize_df(chicago_oclc)\n",
    "cornell_oclc = normalize_df(cornell_oclc)\n",
    "dartmouth_oclc = normalize_df(dartmouth_oclc)\n",
    "jhu_oclc = normalize_df(jhu_oclc)\n",
    "mit_oclc = normalize_df(mit_oclc)\n",
    "princeton_oclc = normalize_df(princeton_oclc)\n",
    "stanford_oclc = normalize_df(stanford_oclc)\n",
    "yale_oclc = normalize_df(yale_oclc)\n",
    "\n",
    "# Exact common intersection\n",
    "common_oclc = (\n",
    "    penn_oclc.select(\"F245\")\n",
    "    .intersect(brown_oclc.select(\"F245\"))\n",
    "    .intersect(chicago_oclc.select(\"F245\"))\n",
    "    .intersect(cornell_oclc.select(\"F245\"))\n",
    "    .intersect(dartmouth_oclc.select(\"F245\"))\n",
    "    .intersect(jhu_oclc.select(\"F245\"))\n",
    "    .intersect(mit_oclc.select(\"F245\"))\n",
    "    .intersect(princeton_oclc.select(\"F245\"))\n",
    "    .intersect(stanford_oclc.select(\"F245\"))\n",
    "    .intersect(yale_oclc.select(\"F245\"))\n",
    ")\n",
    "\n",
    "unique_penn = penn_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_brown = brown_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_chicago = chicago_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_cornell = cornell_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_dartmouth = dartmouth_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_jhu = jhu_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_mit = mit_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_princeton = princeton_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_stanford = stanford_oclc.select(\"F245\").subtract(common_oclc)\n",
    "unique_yale = yale_oclc.select(\"F245\").subtract(common_oclc)\n",
    "\n",
    "print(f\"Number of common records (exact): {common_oclc.count()}\")\n",
    "print(f\"Number of unique records in Penn: {unique_penn.count()}\")\n",
    "print(f\"Number of unique records in Brown: {unique_brown.count()}\")\n",
    "print(f\"Number of unique records in Chicago: {unique_chicago.count()}\")\n",
    "print(f\"Number of unique records in Cornell: {unique_cornell.count()}\")\n",
    "print(f\"Number of unique records in Dartmouth: {unique_dartmouth.count()}\")\n",
    "print(f\"Number of unique records in JHU: {unique_jhu.count()}\")\n",
    "print(f\"Number of unique records in MIT: {unique_mit.count()}\")\n",
    "print(f\"Number of unique records in Princeton: {unique_princeton.count()}\")\n",
    "print(f\"Number of unique records in Stanford: {unique_stanford.count()}\")\n",
    "print(f\"Number of unique records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "# Example fuzzy matching for Penn vs Brown (repeat for others as necessary):\n",
    "threshold = 85\n",
    "common_fuzzy_penn_brown = (\n",
    "    penn_oclc.alias(\"p\")\n",
    "    .crossJoin(brown_oclc.alias(\"b\"))\n",
    "    .withColumn(\"similarity\", fuzzy_match_udf(F.col(\"p.normalized\"), F.col(\"b.normalized\")))\n",
    "    .filter(F.col(\"similarity\") >= threshold)\n",
    ")\n",
    "print(f\"Fuzzy common count Penn-Brown: {common_fuzzy_penn_brown.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy match on title\n",
    "Query marc records for similar titles using fuzzy matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.sql.functions import udf, col, collect_set\n",
    "import re\n",
    "\n",
    "# 1. Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "#spark_df_columbia.createOrReplaceTempView(\"columbia\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_duke.createOrReplaceTempView(\"duke\")\n",
    "#spark_df_harvard.createOrReplaceTempView(\"harvard\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# 2. Query to get 245 field from each table/view and drop duplicates\n",
    "penn_245 = spark.sql(\"SELECT F001, F245, F007 FROM penn\").dropDuplicates([\"F245\"])\n",
    "brown_245 = spark.sql(\"SELECT F001, F245, F007 FROM brown\").dropDuplicates([\"F245\"])\n",
    "chicago_245 = spark.sql(\"SELECT F001, F245, F007 FROM chicago\").dropDuplicates([\"F245\"])\n",
    "# columbia_245 = spark.sql(\"SELECT F001, F245 FROM columbia\").dropDuplicates([\"F245\"])\n",
    "cornell_245 = spark.sql(\"SELECT F001, F245, F007 FROM cornell\").dropDuplicates([\"F245\"])\n",
    "dartmouth_245 = spark.sql(\"SELECT F001, F245, F007 FROM dartmouth\").dropDuplicates([\"F245\"])\n",
    "duke_245 = spark.sql(\"SELECT F001, F245, F007 FROM duke\").dropDuplicates([\"F245\"])\n",
    "# harvard_245 = spark.sql(\"SELECT F001, F245 FROM harvard\").dropDuplicates([\"F245\"])\n",
    "jhu_245 = spark.sql(\"SELECT F001, F245, F007 FROM jhu\").dropDuplicates([\"F245\"])\n",
    "mit_245 = spark.sql(\"SELECT F001, F245, F007 FROM mit\").dropDuplicates([\"F245\"])\n",
    "princeton_245 = spark.sql(\"SELECT F001, F245, F007 FROM princeton\").dropDuplicates([\"F245\"])\n",
    "stanford_245 = spark.sql(\"SELECT F001, F245, F007 FROM stanford\").dropDuplicates([\"F245\"])\n",
    "yale_245 = spark.sql(\"SELECT F001, F245, F007 FROM yale\").dropDuplicates([\"F245\"])\n",
    "\n",
    "# 3. Normalize titles: remove punctuation, lowercase, etc.\n",
    "def normalize(text):\n",
    "    if text:\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "normalize_udf = udf(normalize, StringType())\n",
    "\n",
    "def normalized_df(df):\n",
    "    return df.withColumn(\"normalized\", normalize_udf(F.col(\"F245\")))\n",
    "\n",
    "penn_245 = normalized_df(penn_245)\n",
    "brown_245 = normalized_df(brown_245)\n",
    "chicago_245 = normalized_df(chicago_245)\n",
    "# columbia_245 = normalized_df(columbia_245)\n",
    "cornell_245 = normalized_df(cornell_245)\n",
    "dartmouth_245 = normalized_df(dartmouth_245)\n",
    "duke_245 = normalized_df(duke_245)\n",
    "# harvard_245 = normalized_df(harvard_245)\n",
    "jhu_245 = normalized_df(jhu_245)\n",
    "mit_245 = normalized_df(mit_245)\n",
    "princeton_245 = normalized_df(princeton_245)\n",
    "stanford_245 = normalized_df(stanford_245)\n",
    "yale_245 = normalized_df(yale_245)\n",
    "\n",
    "# 4. Create a simple fingerprint by sorting tokens\n",
    "def fingerprint(text):\n",
    "    if text:\n",
    "        tokens = text.split()\n",
    "        tokens.sort()\n",
    "        return \" \".join(tokens)\n",
    "    return None\n",
    "\n",
    "fingerprint_udf = udf(fingerprint, StringType())\n",
    "\n",
    "# 5. Add source columns and fingerprint column\n",
    "penn_df = penn_245.withColumn(\"source\", F.lit(\"penn\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "brown_df = brown_245.withColumn(\"source\", F.lit(\"brown\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "chicago_df = chicago_245.withColumn(\"source\", F.lit(\"chicago\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "# columbia_df = columbia_245.withColumn(\"source\", F.lit(\"columbia\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "cornell_df = cornell_245.withColumn(\"source\", F.lit(\"cornell\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "dartmouth_df = dartmouth_245.withColumn(\"source\", F.lit(\"dartmouth\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "duke_df = duke_245.withColumn(\"source\", F.lit(\"duke\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "# harvard_df = harvard_245.withColumn(\"source\", F.lit(\"harvard\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "jhu_df = jhu_245.withColumn(\"source\", F.lit(\"jhu\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "mit_df = mit_245.withColumn(\"source\", F.lit(\"mit\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "princeton_df = princeton_245.withColumn(\"source\", F.lit(\"princeton\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "stanford_df = stanford_245.withColumn(\"source\", F.lit(\"stanford\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "yale_df = yale_245.withColumn(\"source\", F.lit(\"yale\")).withColumn(\"fp\", fingerprint_udf(F.col(\"normalized\")))\n",
    "\n",
    "# 6. Union all DataFrames\n",
    "all_df = (penn_df.union(brown_df).union(chicago_df).union(cornell_df)\n",
    "          .union(dartmouth_df).union(jhu_df).union(mit_df).union(princeton_df)\n",
    "          .union(stanford_df).union(yale_df).union(duke_df))\n",
    "\n",
    "# 7. Group by fingerprint and collect the sources where that fingerprint appears\n",
    "grouped = all_df.groupBy(\"fp\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# 8. Define the list of all sources\n",
    "all_sources = {\"penn\", \"brown\", \"chicago\", \"cornell\", \"dartmouth\",\n",
    "               \"duke\", \"jhu\", \"mit\", \"princeton\", \"stanford\", \"yale\"}\n",
    "\n",
    "# 9. Identify fuzzy common records\n",
    "def has_all_sources(sources):\n",
    "    return set(sources) == all_sources\n",
    "\n",
    "has_all_sources_udf = udf(has_all_sources, BooleanType())\n",
    "common_fuzzy = grouped.filter(has_all_sources_udf(F.col(\"sources\")))\n",
    "\n",
    "# 10. Count fuzzy common records\n",
    "common_fuzzy_count = common_fuzzy.count()\n",
    "print(f\"Number of fuzzy common (all sources) records: {common_fuzzy_count}\")\n",
    "\n",
    "# 11. Derive unique records (records not in common fuzzy groups)\n",
    "unique_df = all_df.join(common_fuzzy.select(\"fp\"), on=\"fp\", how=\"left_anti\")\n",
    "\n",
    "# 12. Count unique records per source\n",
    "unique_penn = unique_df.filter(F.col(\"source\") == \"penn\")\n",
    "print(f\"Number of unique fuzzy records in Penn: {unique_penn.count()}\")\n",
    "\n",
    "unique_brown = unique_df.filter(F.col(\"source\") == \"brown\")\n",
    "print(f\"Number of unique fuzzy records in Brown: {unique_brown.count()}\")\n",
    "\n",
    "unique_chicago = unique_df.filter(F.col(\"source\") == \"chicago\")\n",
    "print(f\"Number of unique fuzzy records in Chicago: {unique_chicago.count()}\")\n",
    "\n",
    "unique_cornell = unique_df.filter(F.col(\"source\") == \"cornell\")\n",
    "print(f\"Number of unique fuzzy records in Cornell: {unique_cornell.count()}\")\n",
    "\n",
    "unique_dartmouth = unique_df.filter(F.col(\"source\") == \"dartmouth\")\n",
    "print(f\"Number of unique fuzzy records in Dartmouth: {unique_dartmouth.count()}\")\n",
    "\n",
    "unique_jhu = unique_df.filter(F.col(\"source\") == \"jhu\")\n",
    "print(f\"Number of unique fuzzy records in JHU: {unique_jhu.count()}\")\n",
    "\n",
    "unique_mit = unique_df.filter(F.col(\"source\") == \"mit\")\n",
    "print(f\"Number of unique fuzzy records in MIT: {unique_mit.count()}\")\n",
    "\n",
    "unique_princeton = unique_df.filter(F.col(\"source\") == \"princeton\")\n",
    "print(f\"Number of unique fuzzy records in Princeton: {unique_princeton.count()}\")\n",
    "\n",
    "unique_stanford = unique_df.filter(F.col(\"source\") == \"stanford\")\n",
    "print(f\"Number of unique fuzzy records in Stanford: {unique_stanford.count()}\")\n",
    "\n",
    "unique_yale = unique_df.filter(F.col(\"source\") == \"yale\")\n",
    "print(f\"Number of unique fuzzy records in Yale: {unique_yale.count()}\")\n",
    "\n",
    "#unique_columbia = unique_df.filter(F.col(\"source\") == \"columbia\")\n",
    "#print(f\"Number of unique fuzzy records in Columbia: {unique_columbia.count()}\")\n",
    "\n",
    "unique_duke = unique_df.filter(F.col(\"source\") == \"duke\")\n",
    "print(f\"Number of unique fuzzy records in Duke: {unique_duke.count()}\")\n",
    "\n",
    "#unique_harvard = unique_df.filter(F.col(\"source\") == \"harvard\")\n",
    "#print(f\"Number of unique fuzzy records in Harvard: {unique_harvard.count()}\")\n",
    "\n",
    "# 13. Save the unique records to a file\n",
    "unique_df.select(\"F001\", \"F245\", \"source\").write.mode(\"overwrite\") \\\n",
    "  .parquet(\"/home/jovyan/work/marc/unique_fuzzy_records.parquet\")\n",
    "\n",
    "# 14. Save the common fuzzy records to a file\n",
    "common_fuzzy.select(\"fp\", \"sources\", \"record_count\").write.mode(\"overwrite\") \\\n",
    "  .parquet(\"/home/jovyan/work/marc/common_fuzzy_records.parquet\")\n",
    "\n",
    "# 15. Save the unique records per source to a file\n",
    "unique_penn.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")\n",
    "unique_brown.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_brown.parquet\")\n",
    "unique_chicago.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_chicago.parquet\")\n",
    "unique_cornell.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_cornell.parquet\")\n",
    "unique_dartmouth.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_dartmouth.parquet\")\n",
    "unique_jhu.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_jhu.parquet\")\n",
    "unique_mit.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_mit.parquet\")\n",
    "unique_princeton.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_princeton.parquet\")\n",
    "unique_stanford.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_stanford.parquet\")\n",
    "unique_yale.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_yale.parquet\")\n",
    "# unique_columbia.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_columbia.parquet\")\n",
    "unique_duke.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_duke.parquet\")\n",
    "# unique_harvard.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_harvard.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `F007` cannot be resolved. Did you mean one of the following? [`F001`, `F245`, `fp`, `source`, `normalized`].;\n'Filter NOT (substring('F007, 1, 1) = c)\n+- Relation [fp#8773,F001#8774,F245#8775,normalized#8776,source#8777] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m unique_penn\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_penn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Exclude records where the first character of F007 is \"c\" (electronic resources)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m unique_print_penn \u001b[38;5;241m=\u001b[39m \u001b[43munique_penn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF007\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Now select the rarest titles (ordering by F245 here is only illustrative; adjust as needed)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rarest_titles \u001b[38;5;241m=\u001b[39m unique_print_penn\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF245\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF245\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3138\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3136\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3138\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3143\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `F007` cannot be resolved. Did you mean one of the following? [`F001`, `F245`, `fp`, `source`, `normalized`].;\n'Filter NOT (substring('F007, 1, 1) = c)\n+- Relation [fp#8773,F001#8774,F245#8775,normalized#8776,source#8777] parquet\n"
     ]
    }
   ],
   "source": [
    "# analyze the unique records from penn\n",
    "unique_penn = spark.read.parquet('/home/jovyan/work/marc/unique_penn.parquet')\n",
    "\n",
    "# Create temp view (if needed)\n",
    "unique_penn.createOrReplaceTempView(\"unique_penn\")\n",
    "\n",
    "# Exclude records where the first character of F007 is \"c\" (electronic resources)\n",
    "unique_print_penn = unique_penn.filter(F.substring(F.col(\"F007\"), 1, 1) != \"c\")\n",
    "\n",
    "# Now select the rarest titles (ordering by F245 here is only illustrative; adjust as needed)\n",
    "rarest_titles = unique_print_penn.select(\"F001\", \"F245\").orderBy(F.col(\"F245\").desc())\n",
    "rarest_titles.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
