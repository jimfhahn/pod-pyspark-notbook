{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Step 1: Define Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing\") \\\n",
    "    .setMaster(\"local[36]\") \\\n",
    "    .set(\"spark.executor.memory\", \"300g\") \\\n",
    "    .set(\"spark.driver.memory\", \"300g\") \\\n",
    "    .set(\"spark.driver.maxResultSize\", \"10g\") \n",
    "\n",
    "# Step 2: Initialize SparkContext with the Configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Step 3: Initialize SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install pymarc poetry marctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + '/home/jovyan/.local/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial load only\n",
    "Converts MARC to Parquet format for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import logging\n",
    "from pymarc import Record\n",
    "from pymarc.marcxml import parse_xml_to_array\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_files():\n",
    "    # Use recursive search to pick up .xml files anywhere under the given directory.\n",
    "    # Filter out files that are not MARC XML (e.g. those containing 'opensearch' in their name)\n",
    "    all_files = glob.glob('/home/jovyan/work/*.xml', recursive=True)\n",
    "    files = [f for f in all_files if 'opensearch' not in os.path.basename(f).lower()]\n",
    "    logger.info(f\"Found {len(files)} marc files: {files}\")\n",
    "    return files\n",
    "\n",
    "def process_file(file):\n",
    "    logger.info(f\"Processing file {file}\")\n",
    "\n",
    "    # Define the output directory for Parquet files\n",
    "    output_dir = '/home/jovyan/work/marc/parquet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Determine the output file name: replace .xml with .parquet\n",
    "    base = os.path.basename(file)\n",
    "    output_file = os.path.join(output_dir, base.replace('.xml', '.parquet'))\n",
    "\n",
    "    if file.endswith('.xml'):\n",
    "        # Convert XML to MARC (MRC) format first using parse_xml_to_array.\n",
    "        try:\n",
    "            # Open XML file using utf-8 encoding instead of binary mode.\n",
    "            with open(file, 'r', encoding='utf-8') as f_in:\n",
    "                xml_string = f_in.read()\n",
    "            # Encode to bytes if the parser requires bytes input.\n",
    "            records = parse_xml_to_array(xml_string.encode('utf-8'))\n",
    "            logger.info(f\"Parsed {len(records)} MARC records from XML\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing XML file {file} with utf-8 encoding: {e}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "                temp_file = temp.name\n",
    "            with open(temp_file, 'wb') as f_temp:\n",
    "                for record in records:\n",
    "                    if not isinstance(record, Record):\n",
    "                        raise ValueError(\"Invalid MARC record\")\n",
    "                    f_temp.write(record.as_marc())\n",
    "            logger.info(f\"Wrote temporary MRC file {temp_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting XML to MRC for file {file}: {e}\")\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "            return False\n",
    "\n",
    "        # Run marctable on the temporary MRC file.\n",
    "        logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "        exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "        if exit_status != 0:\n",
    "            logger.error(f\"Error executing marctable command for file {file}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(f\"Created Parquet file {output_file}\")\n",
    "        os.remove(temp_file)\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        # Process non-XML files (for example, raw MRC files).\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "                temp_file = temp.name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating temporary file for {file}: {e}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(file, 'rb') as f_in, open(temp_file, 'wb') as temp_out:\n",
    "                from pymarc import MARCReader\n",
    "                reader = MARCReader(f_in, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "                for record in reader:\n",
    "                    if not isinstance(record, Record):\n",
    "                        raise ValueError(\"Invalid MARC record\")\n",
    "                    temp_out.write(record.as_marc())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {file}: {e}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"Running marctable command: marctable parquet {temp_file} {output_file}\")\n",
    "        exit_status = os.system(f'marctable parquet {temp_file} {output_file}')\n",
    "        if exit_status != 0:\n",
    "            logger.error(f\"Error executing marctable command for file {file}\")\n",
    "            os.remove(temp_file)\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(f\"Created Parquet file {output_file}\")\n",
    "\n",
    "        os.remove(temp_file)\n",
    "        return True\n",
    "\n",
    "def marc2parquet():\n",
    "    files = get_files()\n",
    "    results = []\n",
    "    for file in files:\n",
    "        # Determine output file name based on file extension.\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join('/home/jovyan/work/marc/parquet', base.replace('.xml', '.parquet'))\n",
    "        if os.path.exists(output_file):\n",
    "            logger.info(f\"Skipping already processed file {file}\")\n",
    "            continue\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "    successful_files = sum(results)\n",
    "    logger.info(f\"Processed {len(results)} files, {successful_files} successful, {len(results) - successful_files} failed\")\n",
    "    return all(results)\n",
    "\n",
    "# Run the function and capture the result\n",
    "result = marc2parquet()\n",
    "logger.info(f\"marc2parquet result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet\n",
    "The main benefits of Parquet (like predicate pushdown and column pruning) are realized during file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parquet to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_penn = spark.read.parquet('/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet')\n",
    "spark_df_brown = spark.read.parquet('/home/jovyan/work/marc/parquet/brown-2022-06-14-full-marc21.parquet')\n",
    "spark_df_chicago = spark.read.parquet('/home/jovyan/work/marc/parquet/chicago-2022-06-22-full-marc21.parquet')\n",
    "spark_df_columbia = spark.read.parquet('/home/jovyan/work/marc/parquet/columbia.parquet')\n",
    "spark_df_cornell = spark.read.parquet('/home/jovyan/work/marc/parquet/cornell-2023-04-28-full-marc21.parquet')\n",
    "spark_df_dartmouth = spark.read.parquet('/home/jovyan/work/marc/parquet/dartmouth-2022-08-19-full-marc21.parquet')\n",
    "spark_df_duke = spark.read.parquet('/home/jovyan/work/marc/parquet/duke-2025-01-29-full-marc21.parquet_clean_rev.parquet')\n",
    "spark_df_harvard = spark.read.parquet('/home/jovyan/work/marc/parquet/harvard.parquet')\n",
    "spark_df_jhu = spark.read.parquet('/home/jovyan/work/marc/parquet/jhu-2023-08-23-full-marc21.parquet')\n",
    "spark_df_mit = spark.read.parquet('/home/jovyan/work/marc/parquet/mit-marc21.parquet')\n",
    "spark_df_princeton = spark.read.parquet('/home/jovyan/work/marc/parquet/princeton-2022-06-17-full-marc21.parquet')\n",
    "spark_df_stanford = spark.read.parquet('/home/jovyan/work/marc/parquet/stanford-2024-08-28-full-marc21.parquet')\n",
    "spark_df_yale = spark.read.parquet('/home/jovyan/work/marc/parquet/yale-2022-06-17-full-marc21.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import matching libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (0.18.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find unique by Matchkey and lccn / isbn\n",
    "Matchkey: 1. normalized title 2. edition and 3. publication fields. Matchkey is used when 010 (lccn) or 020 (isbn) fields are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fuzzy common (all sources) records: 49175\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType, ArrayType\n",
    "from pyspark.sql.functions import udf, col, collect_set, explode\n",
    "import re\n",
    "import unicodedata\n",
    "from langdetect import detect\n",
    "\n",
    "# 1. Create temp views\n",
    "spark_df_penn.createOrReplaceTempView(\"penn\")\n",
    "spark_df_brown.createOrReplaceTempView(\"brown\")\n",
    "spark_df_chicago.createOrReplaceTempView(\"chicago\")\n",
    "spark_df_columbia.createOrReplaceTempView(\"columbia\")\n",
    "spark_df_cornell.createOrReplaceTempView(\"cornell\")\n",
    "spark_df_dartmouth.createOrReplaceTempView(\"dartmouth\")\n",
    "spark_df_duke.createOrReplaceTempView(\"duke\")\n",
    "spark_df_harvard.createOrReplaceTempView(\"harvard\")\n",
    "spark_df_jhu.createOrReplaceTempView(\"jhu\")\n",
    "spark_df_mit.createOrReplaceTempView(\"mit\")\n",
    "spark_df_princeton.createOrReplaceTempView(\"princeton\")\n",
    "spark_df_stanford.createOrReplaceTempView(\"stanford\")\n",
    "spark_df_yale.createOrReplaceTempView(\"yale\")\n",
    "\n",
    "# 2. Query to get fields from each view.\n",
    "# Include F010 (LCCN), F020 (ISBN), F245, F250 (edition), and F260 (publication/distribution).\n",
    "penn_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM penn\")\n",
    "brown_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM brown\")\n",
    "chicago_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM chicago\")\n",
    "columbia_245 = spark.sql(\"SELECT F001, F010, F020, F245, F250, F260 FROM columbia\")\n",
    "cornell_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM cornell\")\n",
    "dartmouth_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM dartmouth\")\n",
    "duke_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM duke\")\n",
    "harvard_245 = spark.sql(\"SELECT F001, F010, F020, F245, F250, F260 FROM harvard\")\n",
    "jhu_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM jhu\")\n",
    "mit_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM mit\")\n",
    "princeton_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM princeton\")\n",
    "stanford_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM stanford\")\n",
    "yale_245 = spark.sql(\"SELECT F001, F007, F010, F020, F245, F250, F260 FROM yale\")\n",
    "\n",
    "# 3. Normalize titles (F245), edition (F250), and publication/distribution (F260)\n",
    "# Remove diacritics, punctuation, lowercase, and extra whitespace.\n",
    "def normalize(text):\n",
    "    if text:\n",
    "        # If text is a list, join its elements into a string.\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "        except Exception:\n",
    "            lang = \"unknown\"\n",
    "        if lang != \"en\":\n",
    "            # For non-English, keep diacritics; only lowercase and trim whitespace.\n",
    "            text = text.lower().strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "        else:\n",
    "            # English: remove diacritics and punctuation.\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "            text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "            text = text.lower().strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "    return None\n",
    "\n",
    "normalize_udf = udf(normalize, StringType())\n",
    "\n",
    "def normalized_df(df):\n",
    "    return df.withColumn(\"normalized_title\", normalize_udf(F.col(\"F245\"))) \\\n",
    "             .withColumn(\"normalized_edition\", normalize_udf(F.col(\"F250\"))) \\\n",
    "             .withColumn(\"normalized_pub\", normalize_udf(F.col(\"F260\")))\n",
    "\n",
    "# Apply normalization to each DataFrame\n",
    "penn_245 = normalized_df(penn_245)\n",
    "brown_245 = normalized_df(brown_245)\n",
    "chicago_245 = normalized_df(chicago_245)\n",
    "columbia_245 = normalized_df(columbia_245)\n",
    "cornell_245 = normalized_df(cornell_245)\n",
    "dartmouth_245 = normalized_df(dartmouth_245)\n",
    "duke_245 = normalized_df(duke_245)\n",
    "harvard_245 = normalized_df(harvard_245)\n",
    "jhu_245 = normalized_df(jhu_245)\n",
    "mit_245 = normalized_df(mit_245)\n",
    "princeton_245 = normalized_df(princeton_245)\n",
    "stanford_245 = normalized_df(stanford_245)\n",
    "yale_245 = normalized_df(yale_245)\n",
    "\n",
    "# 4. Create a match key from the normalized title, edition, and publication/distribution.\n",
    "def create_match_key(title, edition, pub):\n",
    "    if title:\n",
    "        key_parts = [title]\n",
    "        if edition:\n",
    "            key_parts.append(edition)\n",
    "        if pub:\n",
    "            key_parts.append(pub)\n",
    "        return \"_\".join(key_parts)\n",
    "    return None\n",
    "\n",
    "match_key_udf = udf(create_match_key, StringType())\n",
    "\n",
    "# 5. Add source column and a match key column\n",
    "penn_df = penn_245.withColumn(\"source\", F.lit(\"penn\")) \\\n",
    "                  .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "brown_df = brown_245.withColumn(\"source\", F.lit(\"brown\")) \\\n",
    "                    .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "chicago_df = chicago_245.withColumn(\"source\", F.lit(\"chicago\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "columbia_df = columbia_245.withColumn(\"source\", F.lit(\"columbia\")) \\\n",
    "                          .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "cornell_df = cornell_245.withColumn(\"source\", F.lit(\"cornell\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "dartmouth_df = dartmouth_245.withColumn(\"source\", F.lit(\"dartmouth\")) \\\n",
    "                            .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "duke_df = duke_245.withColumn(\"source\", F.lit(\"duke\")) \\\n",
    "                  .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "harvard_df = harvard_245.withColumn(\"source\", F.lit(\"harvard\")) \\\n",
    "                        .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "jhu_df = jhu_245.withColumn(\"source\", F.lit(\"jhu\")) \\\n",
    "                .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "mit_df = mit_245.withColumn(\"source\", F.lit(\"mit\")) \\\n",
    "                .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "princeton_df = princeton_245.withColumn(\"source\", F.lit(\"princeton\")) \\\n",
    "                            .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "stanford_df = stanford_245.withColumn(\"source\", F.lit(\"stanford\")) \\\n",
    "                           .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "yale_df = yale_245.withColumn(\"source\", F.lit(\"yale\")) \\\n",
    "                   .withColumn(\"match_key\", match_key_udf(F.col(\"normalized_title\"), F.col(\"normalized_edition\"), F.col(\"normalized_pub\")))\n",
    "\n",
    "# 6. Union all DataFrames\n",
    "all_df = (penn_df.union(brown_df).union(chicago_df).union(cornell_df)\n",
    "          .union(dartmouth_df).union(jhu_df).union(mit_df).union(princeton_df)\n",
    "          .union(stanford_df).union(yale_df).union(duke_df))\n",
    "\n",
    "# 7. Create a helper UDF to extract ISBN and LCCN values as a trimmed array.\n",
    "def get_ids(f010, f020):\n",
    "    ret = []\n",
    "    # Process F010\n",
    "    if f010:\n",
    "        if isinstance(f010, list):\n",
    "            for item in f010:\n",
    "                if item and isinstance(item, str):\n",
    "                    trimmed = item.strip()\n",
    "                    if trimmed:\n",
    "                        ret.append(trimmed)\n",
    "        elif isinstance(f010, str):\n",
    "            trimmed = f010.strip()\n",
    "            if trimmed:\n",
    "                ret.append(trimmed)\n",
    "    # Process F020\n",
    "    if f020:\n",
    "        if isinstance(f020, list):\n",
    "            for item in f020:\n",
    "                if item and isinstance(item, str):\n",
    "                    trimmed = item.strip()\n",
    "                    if trimmed:\n",
    "                        ret.append(trimmed)\n",
    "        elif isinstance(f020, str):\n",
    "            trimmed = f020.strip()\n",
    "            if trimmed:\n",
    "                ret.append(trimmed)\n",
    "    return ret\n",
    "\n",
    "get_ids_udf = udf(get_ids, ArrayType(StringType()))\n",
    "\n",
    "# 8. Add an id_list column (with ISBN and/or LCCN) to all records.\n",
    "all_df = all_df.withColumn(\"id_list\", get_ids_udf(F.col(\"F010\"), F.col(\"F020\")))\n",
    "\n",
    "# 9. Define the matching key array.\n",
    "# If any ISBN or LCCN identifiers exist, use them; otherwise, use the match key.\n",
    "all_df = all_df.withColumn(\"key_array\",\n",
    "    F.when(F.size(F.col(\"id_list\")) > 0, F.col(\"id_list\"))\n",
    "     .otherwise(F.array(F.col(\"match_key\")))\n",
    ")\n",
    "\n",
    "# 10. Explode the key_array so that each record has one key per identifier (or one key if using the match key).\n",
    "all_df_exploded = all_df.withColumn(\"key\", explode(\"key_array\"))\n",
    "\n",
    "# 11. Group by key and collect sources where that key appears.\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# 12. Define the complete set of sources.\n",
    "all_sources = {\"penn\", \"brown\", \"chicago\", \"cornell\", \"dartmouth\",\n",
    "               \"duke\", \"jhu\", \"mit\", \"princeton\", \"stanford\", \"yale\"}\n",
    "\n",
    "# 13. Identify common records: groups where the key appears in all sources.\n",
    "def has_all_sources(sources):\n",
    "    return all(src in sources for src in all_sources)\n",
    "\n",
    "has_all_sources_udf = udf(has_all_sources, BooleanType())\n",
    "common_fuzzy = grouped.filter(has_all_sources_udf(F.col(\"sources\")))\n",
    "\n",
    "# 14. Count common records.\n",
    "common_fuzzy_count = common_fuzzy.count()\n",
    "print(f\"Number of fuzzy common (all sources) records: {common_fuzzy_count}\")\n",
    "\n",
    "# 15. Derive unique records:\n",
    "# Perform an anti-join to remove records that are common across all sources,\n",
    "# then deduplicate based on the match_key so that only one copy of the same book remains.\n",
    "unique_exploded = all_df_exploded.join(common_fuzzy.select(\"key\"), on=\"key\", how=\"left_anti\")\n",
    "unique_df = unique_exploded.dropDuplicates([\"match_key\"])\n",
    "\n",
    "# 16. Count unique records per source.\n",
    "unique_penn = unique_df.filter(F.col(\"source\") == \"penn\")\n",
    "\n",
    "\n",
    "# 17. Save results\n",
    "unique_df.select(\"F001\", \"F245\", \"source\").write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/home/jovyan/work/marc/unique_fuzzy_records.parquet\")\n",
    "common_fuzzy.select(\"key\", \"sources\", \"record_count\").write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/home/jovyan/work/marc/common_fuzzy_records.parquet\")\n",
    "unique_penn.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_penn.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported files per format code successfully.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import col, substring, array_join, udf\n",
    "\n",
    "format_dict = {\n",
    "    'a': \"Map\",\n",
    "    'c': \"Electronic resource\",\n",
    "    'd': \"Globe\",\n",
    "    'f': \"Tactile material\",\n",
    "    'g': \"Projected graphic\",\n",
    "    'h': \"Microform\",\n",
    "    'k': \"Nonprojected graphic\",\n",
    "    'm': \"Motion picture\",\n",
    "    'o': \"Kit\",\n",
    "    'q': \"Notated music\",\n",
    "    'r': \"Remote-sensing image\",\n",
    "    's': \"Sound recording\",\n",
    "    't': \"Text\",\n",
    "    'v': \"Videorecording\",\n",
    "    'z': \"Unspecified\"\n",
    "}\n",
    "\n",
    "def get_F007_desc(code):\n",
    "    return format_dict.get(code, \"Unknown\")\n",
    "\n",
    "get_F007_desc_udf = udf(get_F007_desc, StringType())\n",
    "\n",
    "# Read the parquet\n",
    "unique_penn = spark.read.parquet(\"/home/jovyan/work/marc/unique_penn.parquet\")\n",
    "\n",
    "def convert_array_columns_to_str(df, columns):\n",
    "    for c in columns:\n",
    "        if c in df.columns:\n",
    "            field_type = df.schema[c].dataType\n",
    "            if isinstance(field_type, ArrayType):\n",
    "                # If it's array<string>, convert to semicolon-delimited string\n",
    "                df = df.withColumn(c + \"_str\", array_join(col(c), \"; \")) \\\n",
    "                       .drop(c)\n",
    "            else:\n",
    "                # Otherwise just rename (string columns)\n",
    "                df = df.withColumnRenamed(c, c + \"_str\")\n",
    "    return df\n",
    "\n",
    "# Add any columns that may be arrays\n",
    "columns_to_convert = [\"F007\", \"F010\", \"F020\", \"F250\", \"F260\", \"id_list\", \"key_array\"]  # Add others if needed\n",
    "unique_penn = convert_array_columns_to_str(unique_penn, columns_to_convert)\n",
    "\n",
    "# Extract F007_code from F007_str\n",
    "unique_penn = unique_penn.withColumn(\"F007_code\", substring(col(\"F007_str\"), 1, 1)) \\\n",
    "                         .withColumn(\"F007_desc\", get_F007_desc_udf(col(\"F007_code\")))\n",
    "\n",
    "# Write each format type to a separate CSV\n",
    "for code, description in format_dict.items():\n",
    "    filtered_df = unique_penn.filter(F.col(\"F007_code\") == code)\n",
    "    output_path = f\"/home/jovyan/work/marc/unique_penn_{description.replace(' ', '_')}.csv\"\n",
    "    filtered_df.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "\n",
    "print(\"Exported files per format code successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing Excel for format: Map\n",
      "Finished writing Excel for format: Electronic resource\n",
      "Finished writing Excel for format: Globe\n",
      "Finished writing Excel for format: Tactile material\n",
      "Finished writing Excel for format: Projected graphic\n",
      "Finished writing Excel for format: Microform\n",
      "Finished writing Excel for format: Nonprojected graphic\n",
      "Finished writing Excel for format: Motion picture\n",
      "Finished writing Excel for format: Kit\n",
      "Finished writing Excel for format: Notated music\n",
      "Finished writing Excel for format: Remote-sensing image\n",
      "Finished writing Excel for format: Sound recording\n",
      "Finished writing Excel for format: Text\n",
      "Finished writing Excel for format: Videorecording\n",
      "Finished writing Excel for format: Unspecified\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Format dictionary\n",
    "format_dict = {\n",
    "    'a': \"Map\",\n",
    "    'c': \"Electronic resource\",\n",
    "    'd': \"Globe\",\n",
    "    'f': \"Tactile material\",\n",
    "    'g': \"Projected graphic\",\n",
    "    'h': \"Microform\",\n",
    "    'k': \"Nonprojected graphic\",\n",
    "    'm': \"Motion picture\",\n",
    "    'o': \"Kit\",\n",
    "    'q': \"Notated music\",\n",
    "    'r': \"Remote-sensing image\",\n",
    "    's': \"Sound recording\",\n",
    "    't': \"Text\",\n",
    "    'v': \"Videorecording\",\n",
    "    'z': \"Unspecified\"\n",
    "}\n",
    "\n",
    "# Effective maximum number of data rows per sheet (leaving space for the header)\n",
    "MAX_DATA_ROWS_PER_SHEET = 1048575\n",
    "\n",
    "# Path to where CSV folders are located\n",
    "base_path = \"/home/jovyan/work/marc/\"\n",
    "\n",
    "for code, desc in format_dict.items():\n",
    "    folder_name = f\"unique_penn_{desc.replace(' ', '_')}.csv\"\n",
    "    csv_folder = os.path.join(base_path, folder_name)\n",
    "\n",
    "    # Only include part files (ignore metadata like _SUCCESS)\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, \"part-*.csv\"))\n",
    "    if not csv_files:\n",
    "        continue\n",
    "\n",
    "    # Read and combine CSV partitions\n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df_chunk = pd.read_csv(csv_file, engine=\"python\", on_bad_lines=\"skip\")\n",
    "        df_list.append(df_chunk)\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Determine the number of sheets needed\n",
    "    num_rows = len(combined_df)\n",
    "    sheet_count = (num_rows + MAX_DATA_ROWS_PER_SHEET - 1) // MAX_DATA_ROWS_PER_SHEET\n",
    "\n",
    "    # Create an Excel file and split the DataFrame into sheets\n",
    "    excel_filename = f\"unique_penn_{desc.replace(' ', '_')}.xlsx\"\n",
    "    excel_path = os.path.join(base_path, excel_filename)\n",
    "\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        start_idx = 0\n",
    "        for i in range(sheet_count):\n",
    "            end_idx = min(start_idx + MAX_DATA_ROWS_PER_SHEET, num_rows)\n",
    "            chunk_df = combined_df.iloc[start_idx:end_idx, :]\n",
    "            sheet_name = desc[:20] + f\"_part{i+1}\"  # Truncate sheet name if needed\n",
    "            chunk_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            start_idx = end_idx\n",
    "\n",
    "    print(f\"Finished writing Excel for format: {desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
